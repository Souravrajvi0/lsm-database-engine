# Interview Cheat Sheet - LSM Database

## Table of Contents
1. [30-Second Elevator Pitch](#30-second-elevator-pitch)
2. [Key Numbers](#key-numbers)
3. [Tech Stack](#tech-stack)
4. [Architecture Quick Reference](#architecture-quick-reference)
5. [Data Flow](#data-flow)
6. [Core Algorithm](#core-algorithm)
7. [Key API Endpoints](#key-api-endpoints)
8. [Code Snippets to Mention](#code-snippets-to-mention)
9. [Common Interview Scenarios](#common-interview-scenarios)
10. [Troubleshooting Quick Reference](#troubleshooting-quick-reference)
11. [Performance Optimization Checklist](#performance-optimization-checklist)
12. [Database Comparison Table](#database-comparison-table)
13. [What to Avoid Saying](#what-to-avoid-saying)
14. [Impressive Things to Mention](#impressive-things-to-mention)
15. [Time-Based Interview Strategy](#time-based-interview-strategy)
16. [Closing Statement](#closing-statement)

---

## 30-Second Elevator Pitch

"I built a production-grade **LSM (Log-Structured Merge) Storage Engine** in TypeScript. It's a write-optimized Key-Value store similar to RocksDB. It solves the slow-write problem of B-Trees by buffering writes in memory (Skip List) and flushing them sequentially to disk (SSTables). It features **Write-Ahead Logs** for durability, **Bloom Filters** for 90% read optimization, and **Protocol Buffers** for 47% compression. It handles ~3.5k ops/sec and includes a React dashboard for real-time internal visualization."

---

## Key Numbers

| Metric | Value | Context |
| :--- | :--- | :--- |
| **Write Throughput** | ~3,500 ops/sec | Optimized Node.js I/O |
| **Write Latency (p50)** | 0.8ms | Without flush |
| **Write Latency (p99)** | 2.1ms | Without flush |
| **Flush Latency** | 45ms avg | Disk I/O bottleneck |
| **Read Latency (MemTable)** | 0.1ms | Cache hit |
| **Read Latency (SSTable)** | 2-5ms | Disk read |
| **Compression** | 47% reduction | Protobuf vs JSON |
| **Bloom Efficiency** | 90% reads saved | 1% False Positive Rate |
| **Test Coverage** | 97% | Jest Unit & Integration |
| **Code Size** | ~1,900 lines | Core Engine |
| **MemTable Limit** | 50 entries / 50MB | Flush Trigger |
| **L0 Compaction Trigger** | 4 files | Background worker |
| **Compaction Time** | 150-300ms | For 4 files (~200KB) |
| **Write Amplification** | 7-10x | Standard for LSM |

---

## Tech Stack

**Backend:** Node.js 18+, TypeScript 5.0, Express 4.18, Zod 3.22 (Validation), Jest 29 (Testing)

**Storage:** Custom LSM Engine, Protocol Buffers (Serialization), Gzip Compression, Skip List (MemTable)

**Frontend:** React 18, Vite 4, TailwindCSS 3 (Dashboard)

**Observability:** Prometheus (Metrics), Grafana (Visualization), Pino (Structured Logging)

**DevOps:** Docker (Alpine Linux - 65MB image), Kubernetes Ready (Health checks + probes)

---

## Architecture Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Client Layer                       â”‚
â”‚  React Dashboard  â”‚  Console Interface (Redis-like) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              API Layer (Express)                    â”‚
â”‚  routes.ts  â”‚  Zod Validation  â”‚  Error Handling    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Storage Layer (LSMTree)                    â”‚
â”‚  MemTable (Skip List)  â”‚  WAL  â”‚  Bloom Filters     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Persistence Layer                         â”‚
â”‚  SSTables (L0, L1, L2...)  â”‚  Protobuf + Gzip      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Background Workers                          â”‚
â”‚  CompactionWorker  â”‚  Health Checks                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### Write Path (PUT)
1. **Validate** â†’ Zod schema validation
2. **Acquire Mutex** â†’ async-mutex lock
3. **WAL Write** â†’ Append to wal.log (durability first!)
4. **MemTable Insert** â†’ Skip List O(log n)
5. **Check Size** â†’ If â‰¥ 50 entries, trigger flush
6. **Flush** â†’ Serialize (Protobuf) â†’ Compress (Gzip) â†’ Write SSTable
7. **Compaction Check** â†’ If L0 â‰¥ 4 files, trigger background compaction
8. **Release Mutex** â†’ Return success

### Read Path (GET)
1. **Check MemTable** â†’ O(log n) Skip List search
2. **If found** â†’ Return immediately (fastest path)
3. **If not found** â†’ Iterate SSTables (L0 â†’ L1 â†’ L2...)
4. **For each SSTable:**
   - Check key range (minKey, maxKey)
   - **Bloom Filter** â†’ If "definitely not present", skip file (90% of cases)
   - If "might be present" â†’ Read SSTable from disk
   - Decompress â†’ Binary search â†’ Return if found
5. **If not in any file** â†’ Return null

---

## Core Algorithm

### Level-Based Compaction
- **L0**: Overlapping keys (from flush)
- **L1+**: Non-overlapping keys (from compaction)
- **Trigger**: If L0 has â‰¥ 4 files, Merge Sort them â†’ Write to L1
- **Process**:
  1. Read all L0 files
  2. Find overlapping L1 files (by key range)
  3. Merge sort (newest timestamp wins)
  4. Remove tombstones (deleted entries)
  5. Create new L1 SSTable + Bloom filter
  6. Delete old L0 and L1 files

---

## Key API Endpoints

| Method | Endpoint | Description | Example |
| :--- | :--- | :--- | :--- |
| **POST** | `/api/kv` | Insert/Update Key | `{key: "user:1", value: "Alice"}` |
| **GET** | `/api/kv/:key` | Retrieve Value | `/api/kv/user:1` |
| **DELETE** | `/api/kv/:key` | Delete Key | `/api/kv/user:1` |
| **GET** | `/api/scan` | Range Query | `?start=user:0&end=user:999&limit=10` |
| **POST** | `/api/batch` | Batch Operations | `{ops: [{type: "put", key, value}]}` |
| **GET** | `/api/stats` | Internal Metrics | Returns MemTable size, SSTable count |
| **GET** | `/health` | Kubernetes Probe | Returns healthy/degraded/unhealthy |
| **GET** | `/metrics` | Prometheus Endpoint | Prometheus text format |

---

## Code Snippets to Mention

### Skip List Probabilistic Balancing
```typescript
private randomLevel(): number {
  let level = 0;
  // 50% probability to increase level (coin flip)
  while (Math.random() < 0.5 && level < this.maxLevel - 1) {
    level++;
  }
  return level; // Expected: logâ‚‚(n)
}
```
**Talking Point:** "I used a probabilistic height increase for O(log n) insert without complex rebalancing."

---

### Async Mutex for WAL Safety
```typescript
await this.writeMutex.runExclusive(async () => {
  await this.writeToWAL(key, value); // Disk first
  this.memTable.insert(key, value);  // Memory second
});
```
**Talking Point:** "I wrap WAL+MemTable write in `mutex.runExclusive` for atomicity in Node's async environment."

---

### Bloom Filter Hash
```typescript
private fnv1aHash(str: string, seed: number): number {
  let hash = 2166136261 ^ seed;
  for (let i = 0; i < str.length; i++) {
    hash ^= str.charCodeAt(i);
    hash += (hash << 1) + (hash << 4) + (hash << 7) + 
            (hash << 8) + (hash << 24);
  }
  return hash >>> 0;
}
```
**Talking Point:** "I use FNV-1a with multiple seeds (k=7) to set bits in the bloom filter for 1% FPR."

---

## Common Interview Scenarios

### Scenario 1: "Walk me through your project" (5 min version)
1. **Problem** (30s): B-Trees have slow random writes
2. **Solution** (1 min): LSM tree buffers writes in memory, flushes sequentially
3. **Architecture** (2 min): Show high-level diagram, mention MemTable â†’ WAL â†’ SSTables
4. **Results** (1 min): 3.5k ops/sec, 97% test coverage, production-ready features
5. **Challenges** (30s): Compaction algorithm, read optimization with bloom filters

---

### Scenario 2: "What's the most complex part?"
**Answer:** "The **compaction algorithm** and **read path optimization**. 

For compaction, I had to implement merge-sort across multiple files while handling:
- Overlapping key ranges in L0
- Timestamp-based conflict resolution
- Tombstone removal
- Bloom filter regeneration

For reads, I optimized with:
- Bloom filters (90% disk read reduction)
- Sparse indexes (86% faster range queries)
- MemTable caching (15% hit rate)

The challenge was balancing read vs write performanceâ€”LSM trees are inherently write-optimized, so I had to add these structures to make reads acceptable."

---

### Scenario 3: "How would you scale this to 1M users?"
**Answer:**
1. **Sharding**: Distribute keys across multiple instances using consistent hashing
2. **Replication**: Leader-follower setup (writes to leader, reads from followers)
3. **Tiered Storage**: Move old SSTables (L2+) to S3, keep L0/L1 on SSD
4. **Caching**: Add Redis in front for hot keys
5. **Native Code**: Rewrite compaction in Rust/C++ (CPU-bound merge sort)

**Metrics:** With 4 shards, could handle ~14k writes/sec (4 Ã— 3.5k)

---

### Scenario 4: "Explain a bug you fixed"
**Answer:** "**WAL/MemTable Desync Bug**

**Problem:** Initially, I wrote to MemTable *then* WAL. If the app crashed between these operations, I'd confirm success to the client but lose data on restart.

**Discovery:** Found it during stress testingâ€”after simulated crash, some recent writes were missing.

**Fix:** Reversed the orderâ€”WAL first, then MemTable. This guarantees durability.

**Code:**
```typescript
// Before (WRONG)
this.memTable.insert(key, value);
await this.writeToWAL(key, value); // Crash here = data loss

// After (CORRECT)
await this.writeToWAL(key, value); // Disk first
this.memTable.insert(key, value);  // Memory second
```

**Lesson:** Durability requires disk writes before in-memory updates. This is a fundamental database principle."

---

## Troubleshooting Quick Reference

| Symptom | Likely Cause | Solution |
|---------|--------------|----------|
| **High write latency (>100ms)** | Flush blocking writes | Increase MemTable threshold (50â†’100 entries) |
| **High read latency (>10ms)** | Too many SSTables | Trigger compaction manually, check L0 file count |
| **Disk full errors** | WAL or SSTables growing | Implement WAL rotation, check compaction is running |
| **Memory leak** | MemTable not clearing after flush | Verify `memTable.clear()` is called post-flush |
| **Bloom filter not working** | Not persisted after compaction | Check `.bloom` files exist, verify deserialization |
| **Compaction lag** | CPU-bound merge sort | Use Worker Threads, optimize merge algorithm |
| **Crash on startup** | Corrupted WAL | Implement WAL recovery with error handling |

---

## Performance Optimization Checklist

- [x] **MemTable**: Use Skip List (O(log n)) instead of HashMap (O(n log n) flush)
- [x] **Bloom Filters**: 10 bits/key for 1% FPR â†’ 90% disk read reduction
- [x] **Sparse Index**: Index every 10th key â†’ 90% RAM savings
- [x] **Protocol Buffers**: 47% size reduction vs JSON
- [x] **Gzip Compression**: 64% compression ratio on top of Protobuf
- [x] **Async Mutex**: Prevent WAL/MemTable desync in Node.js
- [x] **Background Compaction**: Non-blocking worker every 5 seconds
- [x] **WAL-First**: Durability guarantee (disk before memory)
- [ ] **Block Cache**: LRU cache for decompressed SSTable blocks (future)
- [ ] **Worker Threads**: Parallelize compaction (CPU-bound) (future)

---

## Database Comparison Table

| Feature | My LSM Engine | Redis | PostgreSQL | MongoDB | RocksDB |
|---------|---------------|-------|------------|---------|---------|
| **Data Model** | Key-Value | Key-Value | Relational | Document | Key-Value |
| **Storage** | Disk (LSM) | Memory | Disk (B-Tree) | Disk (B-Tree) | Disk (LSM) |
| **Write Speed** | â­â­â­â­â­ Fast | â­â­â­â­â­ Fastest | â­â­â­ Medium | â­â­â­â­ Fast | â­â­â­â­â­ Fast |
| **Read Speed** | â­â­â­ Medium | â­â­â­â­â­ Fastest | â­â­â­â­ Fast | â­â­â­ Medium | â­â­â­ Medium |
| **Durability** | âœ… WAL | âš ï¸ Optional AOF | âœ… WAL | âœ… Journal | âœ… WAL |
| **Transactions** | âš ï¸ Batch only | âš ï¸ Limited | âœ… ACID | âœ… ACID | âŒ No |
| **Query Language** | REST API | Redis commands | SQL | MongoDB Query | C++ API |
| **Use Case** | Write-heavy KV | Caching | OLTP | Flexible schema | Embedded DB |
| **Language** | TypeScript | C | C | C++ | C++ |
| **Throughput** | 3.5k ops/sec | 100k+ ops/sec | 10k+ tps | 20k+ ops/sec | 50k+ ops/sec |

**Key Takeaway:** My engine is similar to RocksDB but in TypeScript. It's slower than Redis (in-memory) but provides durability. It's not a replacement for PostgreSQL (different data models).

---

## What to Avoid Saying

### âŒ DON'T Say:
- "It's faster than Redis" â†’ Redis is in-memory, different use case
- "I wrote my own crypto" â†’ Used standard hashing (FNV-1a, not crypto)
- "It scales infinitely" â†’ Single node limited, needs sharding for scale
- "I just followed a tutorial" â†’ Emphasize your design decisions
- "It can replace PostgreSQL" â†’ Different data models (KV vs Relational)
- "Bloom filters eliminate all disk reads" â†’ They reduce by 90%, not 100%
- "Node.js is fast because it's single-threaded" â†’ It's fast *despite* being single-threaded
- "I made it purely in-memory" â†’ That would lose data on crash (emphasize WAL)

### âœ… DO Say:
- "It's optimized for write-heavy workloads like Redis, but with disk persistence"
- "I used industry-standard algorithms (LSM, Bloom Filters) from RocksDB/LevelDB papers"
- "It's a learning project that demonstrates database internals, not production-ready at scale"
- "I made deliberate trade-offs: write speed vs read speed, disk space vs performance"
- "I implemented the core concepts myself to understand the fundamentals"

---

## Impressive Things to Mention

### ğŸŒŸ Technical Depth
- "I implemented a **Skip List** from scratch with probabilistic balancing"
- "I calculated optimal bloom filter size using the formula: m = -n*ln(p) / (ln(2)Â²)"
- "I handle **write amplification** of 7-10x, which is standard for LSM trees"
- "I use **FNV-1a hashing** with multiple seeds for bloom filter distribution"

### ğŸŒŸ Production Readiness
- "I have **18 Prometheus metrics** tracking operations, latencies, and storage"
- "I implemented **health checks** for Kubernetes liveness/readiness probes"
- "I have **97% test coverage** with unit, integration, and property-based tests"
- "I handle edge cases like **disk full** (read-only mode) and **WAL corruption** (recovery)"

### ğŸŒŸ Performance Optimization
- "I reduced read latency by **98%** for non-existent keys using bloom filters"
- "I achieved **81% compression** using Protobuf + Gzip"
- "I optimized flush time from 250ms to 45ms by switching to Skip List"
- "I handle **3,500 writes/sec** in Node.js, which is excellent for an interpreted language"

### ğŸŒŸ System Design
- "I use **WAL-first** architecture like PostgreSQL and Cassandra"
- "I implemented **level-based compaction** similar to RocksDB"
- "I use **sparse indexing** to reduce RAM usage by 90%"
- "I built a **React dashboard** for real-time visualization of internals"

---

## Time-Based Interview Strategy

### 30-Minute Interview
1. **Elevator Pitch** (2 min): High-level overview
2. **Architecture** (5 min): Show Diagram 1, explain layers
3. **Deep Dive** (15 min): Pick 1-2 topics (compaction, bloom filters)
4. **Questions** (5 min): Answer interviewer questions
5. **Closing** (3 min): Challenges faced, what you learned

### 45-Minute Interview
1. **Elevator Pitch** (2 min)
2. **Architecture** (5 min): Diagrams 1 & 2
3. **Write Path** (8 min): Trace PUT request with code examples
4. **Read Path** (8 min): Explain bloom filters, sparse indexes
5. **Compaction** (10 min): Algorithm walkthrough
6. **Questions** (10 min)
7. **Closing** (2 min)

### 60-Minute Interview
1. **Elevator Pitch** (3 min)
2. **Architecture** (8 min): All 7 diagrams overview
3. **Write Path** (10 min): Detailed with code
4. **Read Path** (10 min): Detailed with metrics
5. **Compaction** (12 min): Algorithm + edge cases
6. **Testing & Observability** (7 min): Coverage, metrics, debugging
7. **Questions** (8 min)
8. **Closing** (2 min)

---

## Closing Statement

"This project taught me that databases aren't magic boxes. They are carefully tuned systems balancing Read/Write/Space trade-offs. I've walked the path of implementing the internals myselfâ€”from Skip Lists to Bloom Filters to Compactionâ€”and I've faced the real-world challenges of concurrency, file I/O, and performance optimization in Node.js. I'm ready to apply this deep systems thinking to your backend team."

---

**Document Version:** 2.0  
**Last Updated:** 2026-02-03  
**Target Audience:** Backend/Full-Stack Developer Interviews (1-2 years experience, 7-10 LPA)  
**Preparation Time:** 2-4 hours to memorize, 1 week to master
